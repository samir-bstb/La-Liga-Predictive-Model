import pandas as pd
import numpy as np
import xgboost as xgb
import glob, os
from sklearn.metrics import accuracy_score, log_loss, confusion_matrix
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler  # for normalization

import requests
from io import StringIO

season_urls = {
    '2019-20': 'https://www.football-data.co.uk/mmz4281/1920/SP1.csv',
    '2020-21': 'https://www.football-data.co.uk/mmz4281/2021/SP1.csv',
    '2021-22': 'https://www.football-data.co.uk/mmz4281/2122/SP1.csv',
    '2022-23': 'https://www.football-data.co.uk/mmz4281/2223/SP1.csv',
    '2023-24': 'https://www.football-data.co.uk/mmz4281/2324/SP1.csv',
    '2024-25': 'https://www.football-data.co.uk/mmz4281/2425/SP1.csv'
}

dfs = []
for season, url in season_urls.items():
    print(f"Loading {season} ...")
    r = requests.get(url)
    r.raise_for_status()
    df = pd.read_csv(StringIO(r.text))
    df["season"] = season
    dfs.append(df)

data = pd.concat(dfs, ignore_index=True)
print("Seasons loaded:", data["season"].unique())

data['Date'] = pd.to_datetime(data['Date'], dayfirst=True, errors='coerce') 
data = data.sort_values('Date').reset_index(drop=True)

def build_features(df):
    df = df.copy()
    df["goal_diff"] = df["FTHG"] - df["FTAG"]
    df["result"] = np.where(df["goal_diff"] > 0, "H", np.where(df["goal_diff"] < 0, "A", "D"))

    df["home_points"] = np.where(df["result"] == "H", 3, np.where(df["result"] == "D", 1, 0))
    df["away_points"] = np.where(df["result"] == "A", 3, np.where(df["result"] == "D", 1, 0))

    home_df = df[['Date', 'HomeTeam', 'AwayTeam', 'goal_diff', 'home_points']].rename(columns={'HomeTeam': 'team', 'AwayTeam': 'opponent', 'home_points': 'points'})
    home_df['gd'] = df['goal_diff']
    home_df['venue'] = 'home'

    away_df = df[['Date', 'AwayTeam', 'HomeTeam', 'goal_diff', 'away_points']].rename(columns={'AwayTeam': 'team', 'HomeTeam': 'opponent', 'away_points': 'points'})
    away_df['gd'] = -df['goal_diff']
    away_df['venue'] = 'away'

    team_matches = pd.concat([home_df, away_df])
    team_matches = team_matches.sort_values(['team', 'Date']).reset_index(drop=True)

    # Compute recent gd and points from team's perspective
    team_matches['recent_gd'] = team_matches.groupby('team')['gd'].transform(lambda x: x.rolling(12, min_periods=1).mean().shift(1))
    team_matches['recent_points'] = team_matches.groupby('team')['points'].transform(lambda x: x.rolling(5, min_periods=1).mean().shift(1))

    # Fill NaN for first matches
    team_matches['recent_gd'] = team_matches['recent_gd'].fillna(0)
    team_matches['recent_points'] = team_matches['recent_points'].fillna(0)

    # Merge back to original df for home team
    home_features = team_matches[team_matches['venue'] == 'home'][['Date', 'team', 'recent_gd', 'recent_points']].rename(columns={'recent_gd': 'home_recent_gd', 'recent_points': 'home_recent_points'})
    df = df.merge(home_features, left_on=['Date', 'HomeTeam'], right_on=['Date', 'team'], how='left').drop('team', axis=1)

    # For away team
    away_features = team_matches[team_matches['venue'] == 'away'][['Date', 'team', 'recent_gd', 'recent_points']].rename(columns={'recent_gd': 'away_recent_gd', 'recent_points': 'away_recent_points'})
    df = df.merge(away_features, left_on=['Date', 'AwayTeam'], right_on=['Date', 'team'], how='left').drop('team', axis=1)

    df['h2h_H'] = 0.0
    df['h2h_D'] = 0.0
    df['h2h_A'] = 0.0

    for idx in range(len(df)):
        row = df.iloc[idx]
        home = row['HomeTeam']
        away = row['AwayTeam']
        past = df.iloc[:idx]
        mask1 = (past["HomeTeam"] == home) & (past["AwayTeam"] == away)
        mask2 = (past["HomeTeam"] == away) & (past["AwayTeam"] == home)
        matches = past[mask1 | mask2]
        total = len(matches)
        if total == 0:
            df.at[idx, 'h2h_H'] = 0.333
            df.at[idx, 'h2h_D'] = 0.333
            df.at[idx, 'h2h_A'] = 0.334
            continue
        h_wins = ((matches["HomeTeam"] == home) & (matches["FTHG"] > matches["FTAG"])).sum() + ((matches["AwayTeam"] == home) & (matches["FTAG"] > matches["FTHG"])).sum()
        draws = (matches["FTHG"] == matches["FTAG"]).sum()
        a_wins = total - h_wins - draws
        df.at[idx, 'h2h_H'] = h_wins / total
        df.at[idx, 'h2h_D'] = draws / total
        df.at[idx, 'h2h_A'] = a_wins / total

    df['h2h_H'] *= 1.2
    df['h2h_D'] *= 1.1
    df['h2h_A'] *= 1.3

    features = ["home_recent_gd", "away_recent_gd", "home_recent_points", "away_recent_points", "h2h_H", "h2h_D", "h2h_A"]
    return df, features, team_matches 

def compute_h2h_features(df, home, away, n_matches=10):
    """Returns the proportion of wins, draws, and losses for home vs away in the last n_matches"""
    mask1 = (df["HomeTeam"] == home) & (df["AwayTeam"] == away)
    mask2 = (df["HomeTeam"] == away) & (df["AwayTeam"] == home)
    matches = df[mask1 | mask2].sort_values("Date", ascending=False).head(n_matches)

    total = len(matches)
    if total == 0:
        return {"h2h_H": 0.333, "h2h_D": 0.333, "h2h_A": 0.334}

    h_wins = ((matches["HomeTeam"] == home) & (matches["FTHG"] > matches["FTAG"])).sum() + \
             ((matches["AwayTeam"] == home) & (matches["FTAG"] > matches["FTHG"])).sum()
    draws = (matches["FTHG"] == matches["FTAG"]).sum()
    a_wins = total - h_wins - draws

    return {
        "h2h_H": h_wins / total,
        "h2h_D": draws / total,
        "h2h_A": a_wins / total
    }


data, features, team_matches = build_features(data)  # Get team matches 

# Compute average goals from historical data
avg_home_goals = data['FTHG'].mean()
avg_away_goals = data['FTAG'].mean()

# Latest team features for future predictions
team_latest = team_matches.groupby('team')[['recent_gd', 'recent_points']].last()

# Data Splitting and Label Mapping
train = data[data["season"].isin(["2019-20", "2020-21", "2021-22", "2022-23"])]
valid = data[data["season"] == "2023-24"]
test = data[data["season"] == "2024-25"]

X_train, y_train = train[features], train["result"]
X_valid, y_valid = valid[features], valid["result"]
X_test, y_test = test[features], test["result"]

label_map = {"H": 0, "D": 1, "A": 2}
y_train_num = y_train.map(label_map)
y_valid_num = y_valid.map(label_map)
y_test_num = y_test.map(label_map)

# Inverse for debug
label_map_inverse = {v: k for k, v in label_map.items()}

# Debug: Print class distributions to check imbalance, especially for draws
print("Train distribution:\n", y_train.value_counts(normalize=True))
print("Valid distribution:\n", y_valid.value_counts(normalize=True))
print("Test distribution:\n", y_test.value_counts(normalize=True))

custom_weights = {0: 1.3, 1: 0.75, 2: 1.0}
sample_weights = [custom_weights[label] for label in y_train_num]

# Normalize features to stabilize 
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)
X_test_scaled = scaler.transform(X_test)

dtrain = xgb.DMatrix(X_train_scaled, label=y_train_num, weight=sample_weights)
dvalid = xgb.DMatrix(X_valid_scaled, label=y_valid_num)
dtest = xgb.DMatrix(X_test_scaled, label=y_test_num)

# Model training 
params = {
    "objective": "multi:softprob",
    "num_class": 3,
    "eval_metric": "mlogloss",
    "tree_method": "hist",
    "max_depth": 3,  
    "eta": 0.03,      
    "lambda": 8.0,  
    "alpha": 4.0,   
    "subsample": 0.8, 
    "colsample_bytree": 0.8 
}

evals_result = {}  

model = xgb.train(
    params,
    dtrain,
    num_boost_round=250, 
    evals=[(dtrain, "train"), (dvalid, "valid")],
    early_stopping_rounds=15,  
    evals_result=evals_result
)

# Debug: Plot learning curves
plt.figure(figsize=(10, 6))
plt.plot(evals_result['train']['mlogloss'], label='Train Log Loss')
plt.plot(evals_result['valid']['mlogloss'], label='Valid Log Loss')
plt.xlabel('Boosting Rounds')
plt.ylabel('Log Loss')
plt.title('Learning Curves')
plt.legend()
plt.grid(True)
plt.show()

# Debug: Plot feature importance 
try:
    xgb.plot_importance(model, importance_type='gain')
    plt.title('Feature Importance (Gain)')
    plt.show()
except ValueError as e:
    print(f"Error in plotting importance: {e}. Model may have no trees.")

# Prediction Functions 
def predict_match(home, away, df, model, features, scaler):
    home_recent_gd = team_latest.loc[home, 'recent_gd'] if home in team_latest.index else 0
    home_recent_points = team_latest.loc[home, 'recent_points'] if home in team_latest.index else 0
    away_recent_gd = team_latest.loc[away, 'recent_gd'] if away in team_latest.index else 0
    away_recent_points = team_latest.loc[away, 'recent_points'] if away in team_latest.index else 0

    h2h = compute_h2h_features(df, home, away, n_matches=10)

    h2h["h2h_H"] *= 1.2
    h2h["h2h_D"] *= 1.1
    h2h["h2h_A"] *= 1.3

    row = {
        "home_recent_gd": home_recent_gd,
        "away_recent_gd": away_recent_gd,
        "home_recent_points": home_recent_points,
        "away_recent_points": away_recent_points,
        "h2h_H": h2h["h2h_H"],
        "h2h_D": h2h["h2h_D"],
        "h2h_A": h2h["h2h_A"]
    }

    X = pd.DataFrame([row])[features]
    X_scaled = scaler.transform(X) 
    dX = xgb.DMatrix(X_scaled)
    probs = model.predict(dX)[0]  
    return {"H": probs[0], "D": probs[1], "A": probs[2]}

probs = predict_match("Real Madrid", "Barcelona", data, model, features, scaler)
formatted_probs = {k: f"{v * 100:.1f}%" for k, v in probs.items()}
home, away = "Real Madrid", "Barcelona"
print(f"{home} vs {away}")
for k, v in formatted_probs.items():
    print(f"{k}: {v}")

# Simulation function 
def simulate_season_realistic(season_df, model, features, lambda_h=avg_home_goals, lambda_a=avg_away_goals, scaler=scaler):
    teams = sorted(set(season_df["HomeTeam"]).union(set(season_df["AwayTeam"])))
    table = {t: {"points": 0, "GF": 0, "GA": 0, "GD": 0} for t in teams}

    for _, row in season_df.iterrows():
        home, away = row["HomeTeam"], row["AwayTeam"] 
        X_row = pd.DataFrame([row[features]])
        X_row_scaled = scaler.transform(X_row)
        dX = xgb.DMatrix(X_row_scaled)
        probs_arr = model.predict(dX)[0]
        probs = {"H": probs_arr[0], "D": probs_arr[1], "A": probs_arr[2]}

        p = np.array([probs["H"], probs["D"], probs["A"]])
        p = p / np.sum(p)

        # Sample outcome based on probabilities using Poisson
        outcome = np.random.choice(["H", "D", "A"], p=p)

        # Sample goals conditional on outcome using Poisson
        while True:
            home_goals = np.random.poisson(lambda_h)
            away_goals = np.random.poisson(lambda_a)
            if (outcome == "H" and home_goals > away_goals) or \
               (outcome == "D" and home_goals == away_goals) or \
               (outcome == "A" and home_goals < away_goals):
                break

        # Update table
        table[home]["GF"] += home_goals
        table[home]["GA"] += away_goals
        table[away]["GF"] += away_goals
        table[away]["GA"] += home_goals

        if outcome == "H":
            table[home]["points"] += 3
        elif outcome == "A":
            table[away]["points"] += 3
        else:
            table[home]["points"] += 1
            table[away]["points"] += 1

    # Compute GD
    for t in teams:
        table[t]["GD"] = table[t]["GF"] - table[t]["GA"]

    # Sort by points and GD
    table_sorted = dict(sorted(table.items(), key=lambda x: (x[1]["points"], x[1]["GD"]), reverse=True))
    return table_sorted

season_df = data[data["season"] == "2024-25"]
season_table = simulate_season_realistic(season_df, model, features)

print("La Liga Season 24/25 Table (Standard prediction)")
for team, stats in season_table.items():
    print(f"{team}: Pts={stats['points']}, GF={stats['GF']}, GA={stats['GA']}, GD={stats['GD']}")
print()

def simulate_season_with_predict_match(season_df, model, features, lambda_h=avg_home_goals, lambda_a=avg_away_goals, scaler=scaler):
    # Sort by date to simulate chronologically
    season_df = season_df.sort_values('Date').reset_index(drop=True)

    teams = sorted(set(season_df["HomeTeam"]).union(set(season_df["AwayTeam"])))
    table = {t: {"points": 0, "GF": 0, "GA": 0, "GD": 0} for t in teams}

    for _, row in season_df.iterrows():
        home, away = row["HomeTeam"], row["AwayTeam"]

        # Call predict_match function
        probs = predict_match(home, away, data, model, features, scaler)

        # Normalize probabilities
        p = np.array([probs["H"], probs["D"], probs["A"]])
        p = p / np.sum(p)

        outcome = np.random.choice(["H", "D", "A"], p=p)

        # Sample goals conditional on outcome using Poisson
        while True:
            home_goals = np.random.poisson(lambda_h)
            away_goals = np.random.poisson(lambda_a)
            if (outcome == "H" and home_goals > away_goals) or \
               (outcome == "D" and home_goals == away_goals) or \
               (outcome == "A" and home_goals < away_goals):
                break

        # Update table
        table[home]["GF"] += home_goals
        table[home]["GA"] += away_goals
        table[away]["GF"] += away_goals
        table[away]["GA"] += home_goals

        if outcome == "H":
            table[home]["points"] += 3
        elif outcome == "A":
            table[away]["points"] += 3
        else:
            table[home]["points"] += 1
            table[away]["points"] += 1

    # Get GD
    for t in teams:
        table[t]["GD"] = table[t]["GF"] - table[t]["GA"]

    # Sort by points and GD
    table_sorted = dict(sorted(table.items(), key=lambda x: (x[1]["points"], x[1]["GD"]), reverse=True))

    # Display the table
    print("La Liga Season 24/25 Table (With predict match function)")
    for team, stats in table_sorted.items():
        print(f"{team}: Pts={stats['points']}, GF={stats['GF']}, GA={stats['GA']}, GD={stats['GD']}")
    print()

    return table_sorted

season_table_predict = simulate_season_with_predict_match(season_df, model, features)

# Model Evaluation
def evaluate_model(model, X, y_true, scaler):
    X_scaled = scaler.transform(X)
    dX = xgb.DMatrix(X_scaled)
    y_pred_probs = model.predict(dX)
    y_pred = np.argmax(y_pred_probs, axis=1)
    y_true_num = y_true.map(label_map)  
    acc = accuracy_score(y_true_num, y_pred)
    ll = log_loss(y_true_num, y_pred_probs)
    cm = confusion_matrix(y_true_num, y_pred)
    return {"accuracy": acc, "log_loss": ll, "confusion_matrix": cm}

print("Training metrics:", evaluate_model(model, X_train, y_train, scaler))
print("Validation metrics:", evaluate_model(model, X_valid, y_valid, scaler))
print("Testing metrics:", evaluate_model(model, X_test, y_test, scaler))

# Debug print average weight per class
unique_classes, counts = np.unique(y_train_num, return_counts=True)
for cls, count in zip(unique_classes, counts):
    class_mask = (y_train_num == cls)
    avg_weight = np.mean(np.array(sample_weights)[class_mask]) if class_mask.sum() > 0 else 0
    print(f"Average weight for class {cls} ({label_map_inverse[cls]}): {avg_weight:.4f}")
